**Tokenizer Refactoring Plan**

**Objective:** Refactor the existing `tokenizer.asm` to separate the tokenization/parsing logic from the execution logic.

**Phase 1: Create the New Tokenizer (`tokenizer1.asm`)**

1.  **Define Token Representation:**
    *   Create two new memory locations to store the output of the tokenizer:
        *   `$TOKEN_TYPE`: A byte to hold the type of the current token.
        *   `$TOKEN_VALUE`: A word to hold the value associated with the token.
    *   Define constants for the token types:
        *   `~TOKEN_NONE 0` (No token found / end of input)
        *   `~TOKEN_UNKNOWN 1` (Token is not a command, number, or variable)
        *   `~TOKEN_CMD 2` (Token is a command)
        *   `~TOKEN_NUM 3` (Token is a number)
        *   `~TOKEN_VAR 4` (Token is a variable)

2.  **Implement the Tokenizer Routine (`@get_next_token`):**
    *   Create a new primary function in `tokenizer.asm` named `@get_next_token`.
    *   This function will scan the input buffer (e.g., `$CMD_BUFFER`) one token at a time. It will need its own pointer to keep track of the scan position.
    *   For each token it finds, it will perform classification:
        *   **Command:** If the token matches a string in the command lookup table (`$STR_TABLE`), set `$TOKEN_TYPE` to `~TOKEN_CMD` and `$TOKEN_VALUE` to the address of the corresponding command handler.
        *   **Variable:** If the token is a single uppercase letter, set `$TOKEN_TYPE` to `~TOKEN_VAR` and `$TOKEN_VALUE` to its ASCII value.
        *   **Number:** If the token is identified as a number by the `@is_numeric` logic, set `$TOKEN_TYPE` to `~TOKEN_NUM` and `$TOKEN_VALUE` to the converted integer.
        *   **Unknown:** If none of the above, set `$TOKEN_TYPE` to `~TOKEN_UNKNOWN`.
    *   The function will return after processing one token, leaving the results in the new memory locations.

**Phase 2: Create the New Executor (`kernel_cliV4.asm`)**

1.  **Create the Executor Loop (`@execute_command_buffer`):**
    *   In `kernel_cliV4.asm`, create a new routine named `@execute_command_buffer`.
    *   This routine will replace the single call to the old `@tokenize_and_execute`.
    *   It will loop, calling `@get_next_token` on each iteration.

2.  **Implement the Execution Logic:**
    *   Inside the loop, after calling `@get_next_token`, it will inspect `$TOKEN_TYPE`.
    *   It will use a series of comparisons to decide what action to take (a "switch-case" structure):
        *   If `~TOKEN_CMD`, it will `call` the handler address stored in `$TOKEN_VALUE`.
        *   If `~TOKEN_NUM` or `~TOKEN_VAR`, it will push the value from `$TOKEN_VALUE` onto the data stack.
        *   If `~TOKEN_UNKNOWN`, it will call an error-reporting routine.
        *   If `~TOKEN_NONE`, it will exit the loop, as the command has been fully processed.

**Phase 3: Integration and Cleanup**

1.  **Update the CLI:** The main CLI processing routine (`:cli_process_command_buffer`) will be updated to call the new `@execute_command_buffer` instead of the old tokenizer.
2.  **Update the STACKS Interpreter:** The `run` command within `interpreter.asm`, which also currently uses `@tokenize_and_execute`, must be updated to use the new two-phase process. This ensures both the interactive CLI and the STACKS interpreter benefit from the new, cleaner architecture.
3.  **Deprecate Old Code:** The original, monolithic `@tokenize_and_execute` routine in `tokenizer.asm` will be removed.