building a single perceptron for a separator task
in a way the code can be reused for a larger neural network

goal: prove its working on my Stern-XT/Stacks project


for reusebility we neet an neural network libray: nn_lib.stack
Functions of that library:
    - a train function
    - a predict function (which performs a weighted sum / dot product)
    - a constructor function (e.g., new_perceptron)
    - ......

The nn_lib is using other libraries:
    - Fixed_point_lib
    - We are gonne use the heap array type, with the following structure:
        - A `weights_array` (capacity 2) to hold the two fixed-point weights (one for x, one for y).
        - A `perceptron_array` (capacity 2) to represent a single neuron/perceptron:
            - Index 0: The fixed-point `bias` value.
            - Index 1: The `pointer` to the weights_array.
        - An `input_array` (capacity 2) to hold the x,y coordinates of a data point.

    Total heap usage:
        - 3 arrays
        - 12 words of memory
        - This is sufficient for the entire training process, as training updates data in-place.


### Application Main Loop (Training Process)

The application will run in two distinct phases: a Training Phase and a Testing Phase.

#### Phase 1: Training Loop
This loop runs for a fixed number of iterations (e.g., 1000) to teach the model.
1.  **Generate Data:** Create a random point (`x`, `y`).
2.  **Predict:** Get the model's `guess` using `NN.predict`.
3.  **Calculate Truth & Error:** Determine the `true_label` and the `error`.
4.  **Train (Adjust):** If `error` is not zero, call `NN.train_step` to update weights and bias.
5.  **Visualize:** Plot the point (e.g., green for correct, red for incorrect).

#### Phase 2: Testing Loop ("New Plot")
This loop runs after training is complete to evaluate the "frozen" model.
1.  **Generate Data:** Create a new random point (`x`, `y`).
2.  **Predict:** Get the model's `guess` using `NN.predict`.
3.  **(No Training):** Do NOT call `NN.train_step`. The model's weights are no longer changed.
4.  **Visualize:** Plot the point using a different color scheme (e.g., blue for correct, orange for incorrect) to show how the trained model performs on new data.







### Future Expansion to a Multi-Layer Network

To expand this project into a larger model, the following features/modifications would be needed:

1.  **Hierarchical Data Structures:** Introduce `Layer` (an array of neuron pointers) and `Network` (an array of layer pointers) concepts.
2.  **Backpropagation Algorithm:** Implement a new `train_network` function that uses backpropagation to adjust weights in hidden layers. This is the most significant change.
3.  **New Activation Functions:** The simple Step function is not enough. Implement differentiable functions like Sigmoid or ReLU using the `fixed_point_lib`.
4.  **Matrix Operations:** For performance, enhance `std_heap.stacks` with `MATRIX.get` and `MATRIX.put` to allow storing a layer's weights in a single, efficient matrix.


### Feasibility on the Stern-XT Platform

This project, while ambitious, is entirely feasible on the Stern-XT platform due to a combination of clever software design and foundational libraries.

*   **Integer-Only Math is Sufficient:** The Stern-XT CPU performs integer arithmetic. The `fixed_point_lib` brilliantly bridges the gap by simulating fractional numbers, allowing us to represent weights, biases, and learning rates accurately.
*   **Dynamic Memory Allocation:** A neural network requires dynamic data structures. The `std_heap` library, with its `ARRAY` functionality, provides the crucial ability to allocate memory for neurons and their weights on the heap as the program runs.
*   **High-Level Abstraction:** The Stacks language and its compiler allow us to write complex logic (like the training loop and prediction functions) in a structured, readable way, abstracting away the low-level machine instructions.
*   **Essential Primitives:** Core functions like `RND` are available for necessary tasks like initializing the perceptron's weights with small random values to break symmetry and start the learning process.

In essence, while the hardware is simple, the software ecosystem built on top of it is powerful enough to tackle advanced concepts like neural networks.



The Future starts here:

    Creating an multilayer Neural Network

    Neural Network dimensions
     2 to 8         input neurons
     4              hidden neurons
     4              output neurons

    To not to break the past we create a new library, called mlnn_lib.stacks



    # Design Document: Multi-Layer Neural Network Library (mlnn_lib)

## 1. Project Goal

To create a new library, `mlnn_lib.stacks`, capable of constructing, training, and running a multi-layer perceptron (MLP) on the Stern-XT platform. This library will be the foundation for creating a game AI with an 8-input, 4-output neural network.

The project will be divided into two main parts:
1.  **The Library (`mlnn_lib.stacks`):** Provides the core MLP functionality.
2.  **The Trainer Application:** A separate program to train the network and save its weights.


## 2. Hierarchical Data Structures

The network will be built using a simplified, fixed 3-layer structure, managed by the `std_heap` library. This avoids the complexity of nested "arrays of arrays".

*   **`Perceptron`:** Unchanged. A pointer to an array of `[bias, weights_ptr]`.
*   **`Layer`:** A pointer to an array of `perceptron_ptr`s.
*   **`Network`:** The top-level structure. This will be a simple array containing pointers to the two trainable layers: `[hidden_layer_ptr, output_layer_ptr]`. This fixed structure is much simpler than a dynamic array of layers.

This fixed 3-layer structure (input -> hidden -> output) is sufficient for the project goals and dramatically simplifies implementation.

**Memory Management Note:** The `std_heap` library's `ARRAY.new` function allocates new memory from the heap but does not provide a way to free or clear an existing array. To reuse temporary arrays (e.g., for storing activations during a forward pass), a helper function will be needed to reset the array's `Count` field to 0. This avoids consuming the entire heap with temporary allocations during training.


## 3. Core Algorithms

The input layer is not explicitly stored as a `Layer` structure, as it has no neurons with weights; its values are provided by the `input_array` at runtime.
### 3.1. Forward Pass (Prediction)

The `NN.predict` function will perform a forward pass through the entire network.

1.  The initial `input_array` is fed into the first hidden layer.
2.  For each subsequent layer, the *output* (activations) of the previous layer becomes the *input*.
3.  The final output of the last layer is the network's prediction.
The `predict` function will first calculate the hidden layer's activations, then use those activations as input to calculate the final output layer's activations.
### 3.2. Backward Pass (Training - Backpropagation)

This is the most significant new development. The `NN.train` function will implement the backpropagation algorithm.

1.  **Perform a Forward Pass:** Execute a full forward pass, but critically, **store the output (activation) of every neuron in every layer**. These intermediate values are required for the backward pass.
2.  **Calculate Output Layer Error:** For the final output layer, calculate the error gradient. This is `(target - actual) * derivative_of_activation(output)`.
3.  **Propagate Error Backwards:** For each hidden layer (moving from back to front):
    *   Calculate the error gradient for each neuron based on the weighted sum of the errors from the layer *in front* of it.
    *   The formula involves the derivative of the activation function. This step will be specific to back-propagating from the output layer to the single hidden layer.
4.  **Update All Weights:** Once all error gradients are calculated, loop through every neuron in the network and update its weights and bias using the standard rule: `new_weight = old_weight + learning_rate * gradient * input`.


The simple step function is not sufficient for backpropagation. We need a differentiable activation function.

*   **Proposed Function: ReLU (Rectified Linear Unit)**
    *   **Formula:** `f(x) = max(0, x)`
    *   **Implementation:** `DUP 0 < IF 0 END`
    *   **Derivative:** `f'(x) = 1 if x > 0, else 0`
    *   **Reasoning:** ReLU is computationally very cheap (no exponentials or divisions) and performs well, making it ideal for the Stern-XT's CPU.

We will need to create two new helper functions in the library: `_relu` and `_relu_derivative`.


## 4. Proposed Library API (`mlnn_lib.stacks`)

The new library will have a clear, high-level API.

*   `DEF NN.new_network { (input_size, hidden_size, output_size -- network_ptr) }`
    *   A simplified constructor that takes the number of neurons for each of the three layers. It will allocate the hidden layer, output layer, and the main network structure on the heap.

*   `DEF NN.predict { (network_ptr, input_array_ptr -- output_array_ptr) }`
    *   Performs the forward pass and returns a pointer to an array containing the final output values.

*   `DEF NN.train { (network_ptr, input_array_ptr, target_array_ptr, learning_rate -- ) }`
    *   Executes one full step of the backpropagation algorithm.


## 6. Proof of Concept: Trainer Application

To prove that the `mlnn_lib` is working correctly, the project will include a trainer application that solves the classic XOR problem. This application will demonstrate the entire lifecycle from creation to a trained state in a single run.

The trainer application (`nn_xor_trainer.stacks`) will:
1.  **Build the Network:** Use `NN.new_network` to create a small network suitable for the task, such as a `2-4-1` architecture (2 inputs, 1 hidden layer of 4 neurons, 1 output).
2.  **Predict (Untrained):** Immediately test the new, untrained network by running `NN.predict` on all four XOR inputs. The results, which will be random/incorrect, should be printed to establish a baseline.
3.  **Train the Network:** Run a training loop for a fixed number of iterations. In each iteration, it will:
    *   Randomly pick one of the four XOR data points (e.g., `[0, 1]`).
    *   Provide the corresponding target output (e.g., `[1]`).
    *   Call `NN.train` to adjust the network's weights.
4.  **Predict (Trained):** After the training loop is complete, test the now-trained network by running `NN.predict` again on all four XOR inputs. The new, correct results should be printed to demonstrate that learning has occurred.

This "all-in-one" approach is ideal for a proof-of-concept. It avoids the complexity of saving and loading network weights, while providing a clear and definitive demonstration of the library's ability to learn.


## 7. Stacks Language Implementation Notes

To avoid potential misunderstandings during implementation, it's important to remember the following characteristics of the Stacks language regarding variables:

*   **Global Scope:** The Stacks language does not support local scopes. All variables are globally accessible once they are declared.
*   **Compile-Time Initialization:** Variables must be declared and initialized at compile-time using the `VALUE` instruction (e.g., `VALUE my_var 0`). This allocates space for the variable and sets its initial value.
*   **Runtime Assignment:** To change a variable's value during program execution, the `AS` instruction must be used. The value to be assigned is pushed onto the stack first, followed by the `AS` keyword and the variable name (e.g., `10 AS my_var`).



## 8. Development Plan & Testing Strategy

To ensure a robust and manageable development process, the project will be implemented in three distinct, testable phases:

### Phase 1: Network Construction (`NN.new_network`)
1.  **Implementation:** Create the `NN.new_network` function. This function will be responsible for allocating all necessary heap memory for a complete 3-layer network, including the main network array, the layer arrays, all perceptrons, and their corresponding weight arrays.
2.  **Testing:** Create a simple test program that calls `NN.new_network` (e.g., for a 2-2-1 network). The test will verify the correctness of the allocated data structures by traversing the pointers and checking that the capacities and counts of all created arrays are correct.

### Phase 2: Forward Pass (`NN.predict`)
1.  **Implementation:**
    *   Create and unit-test the helper functions `_relu` and `_relu_derivative`.
    *   Implement the `NN.predict` function, which performs a full forward pass through the network.
2.  **Testing:** Use a network created in Phase 1. Manually set all weights and biases to known values (e.g., 0.5, 1.0). Provide a known input array and calculate the expected output on paper. The test will pass if the output from `NN.predict` matches the manually calculated result.

### Phase 3: Backpropagation (`NN.train`)
1.  **Implementation:** Implement the `NN.train` function, which executes one full cycle of backpropagation: forward pass, error calculation, and weight updates.
2.  **Testing:** The "Proof of Concept" XOR trainer application will serve as the test for this phase. Successfully training a network to solve the XOR problem will be the definitive validation that the `train` function is working correctly.




## 9. Code Snippets & Examples
### ARRAY.clear
    # ----------------------------------------------------------------------------
    # DEF ARRAY.clear (Clear Array Method)
    # Resets the count of an array to zero, effectively clearing it without
    # deallocating its memory.
    #
    # @param array_ptr A pointer to an array structure.
    # ----------------------------------------------------------------------------
    DEF ARRAY.clear {
        # ( array_ptr -- )
        # This function takes an array pointer, calculates the address
        # of its 'count' field (ptr + 1), and writes a 0 to that address
        # using a temporary pointer variable.
        1 + AS _temp_ptr
        0 AS *_temp_ptr
    }