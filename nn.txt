building an single perceptron for an seperator task
in a way the code can be reused for an lager neural network

goal: prove its working on my Stern-XT/Stacks project


for reusebility we neet an neural network libray: nn_lib.stack
Functions of that library:
    - a train function
    - a predict function (which performs a weighted sum / dot product)
    - a constructor function (e.g., new_perceptron)
    - ......

The nn_lib is using other libraries:
    - Fixed_point_lib
    - We are gonne use the heap array type, with the following structure:
        - A `weights_array` (capacity 2) to hold the two fixed-point weights (one for x, one for y).
        - A `perceptron_array` (capacity 2) to represent a single neuron/perceptron:
            - Index 0: The fixed-point `bias` value.
            - Index 1: The `pointer` to the weights_array.
        - An `input_array` (capacity 2) to hold the x,y coordinates of a data point.

    Total heap usage:
        - 3 arrays
        - 12 words of memory
        - This is sufficient for the entire training process, as training updates data in-place.


### Application Main Loop (Training Process)

The application will run in two distinct phases: a Training Phase and a Testing Phase.

#### Phase 1: Training Loop
This loop runs for a fixed number of iterations (e.g., 1000) to teach the model.
1.  **Generate Data:** Create a random point (`x`, `y`).
2.  **Predict:** Get the model's `guess` using `NN.predict`.
3.  **Calculate Truth & Error:** Determine the `true_label` and the `error`.
4.  **Train (Adjust):** If `error` is not zero, call `NN.train_step` to update weights and bias.
5.  **Visualize:** Plot the point (e.g., green for correct, red for incorrect).

#### Phase 2: Testing Loop ("New Plot")
This loop runs after training is complete to evaluate the "frozen" model.
1.  **Generate Data:** Create a new random point (`x`, `y`).
2.  **Predict:** Get the model's `guess` using `NN.predict`.
3.  **(No Training):** Do NOT call `NN.train_step`. The model's weights are no longer changed.
4.  **Visualize:** Plot the point using a different color scheme (e.g., blue for correct, orange for incorrect) to show how the trained model performs on new data.







### Future Expansion to a Multi-Layer Network

To expand this project into a larger model, the following features/modifications would be needed:

1.  **Hierarchical Data Structures:** Introduce `Layer` (an array of neuron pointers) and `Network` (an array of layer pointers) concepts.
2.  **Backpropagation Algorithm:** Implement a new `train_network` function that uses backpropagation to adjust weights in hidden layers. This is the most significant change.
3.  **New Activation Functions:** The simple Step function is not enough. Implement differentiable functions like Sigmoid or ReLU using the `fixed_point_lib`.
4.  **Matrix Operations:** For performance, enhance `std_heap.stacks` with `MATRIX.get` and `MATRIX.put` to allow storing a layer's weights in a single, efficient matrix.


### Feasibility on the Stern-XT Platform

This project, while ambitious, is entirely feasible on the Stern-XT platform due to a combination of clever software design and foundational libraries.

*   **Integer-Only Math is Sufficient:** The Stern-XT CPU performs integer arithmetic. The `fixed_point_lib` brilliantly bridges the gap by simulating fractional numbers, allowing us to represent weights, biases, and learning rates accurately.
*   **Dynamic Memory Allocation:** A neural network requires dynamic data structures. The `std_heap` library, with its `ARRAY` functionality, provides the crucial ability to allocate memory for neurons and their weights on the heap as the program runs.
*   **High-Level Abstraction:** The Stacks language and its compiler allow us to write complex logic (like the training loop and prediction functions) in a structured, readable way, abstracting away the low-level machine instructions.
*   **Essential Primitives:** Core functions like `RND` are available for necessary tasks like initializing the perceptron's weights with small random values to break symmetry and start the learning process.

In essence, while the hardware is simple, the software ecosystem built on top of it is powerful enough to tackle advanced concepts like neural networks.



The Future starts here:

    Creating an multilayer Neural Network

    Neural Network dimensions
     2 to 8         input neurons
     4              hidden neurons
     4              output neurons

    To not to break the past we create an new libray, called mlnn_lib.stacks



    # Design Document: Multi-Layer Neural Network Library (mlnn_lib)

## 1. Project Goal

To create a new library, `mlnn_lib.stacks`, capable of constructing, training, and running a multi-layer perceptron (MLP) on the Stern-XT platform. This library will be the foundation for creating a game AI with an 8-input, 4-output neural network.

The project will be divided into two main parts:
1.  **The Library (`mlnn_lib.stacks`):** Provides the core MLP functionality.
2.  **The Trainer Application:** A separate program to train the network and save its weights.


## 2. Hierarchical Data Structures

The network will be built using nested arrays from the `std_heap` library. This extends the design of the single perceptron.

*   **`Perceptron`:** Unchanged. A pointer to an array of `[bias, weights_ptr]`.
*   **`Layer`:** A new structure. A pointer to an array of `perceptron_ptr`s.
*   **`Network`:** The top-level structure. A pointer to an array of `layer_ptr`s.

This structure allows for flexible network architectures.


## 3. Core Algorithms

### 3.1. Forward Pass (Prediction)

The `MLNN.predict` function will perform a forward pass through the entire network.

1.  The initial `input_array` is fed into the first hidden layer.
2.  For each subsequent layer, the *output* (activations) of the previous layer becomes the *input*.
3.  The final output of the last layer is the network's prediction.

### 3.2. Backward Pass (Training - Backpropagation)

This is the most significant new development. The `MLNN.train` function will implement the backpropagation algorithm.

1.  **Perform a Forward Pass:** Execute a full forward pass, but critically, **store the output (activation) of every neuron in every layer**. These intermediate values are required for the backward pass.
2.  **Calculate Output Layer Error:** For the final output layer, calculate the error gradient. This is `(target - actual) * derivative_of_activation(output)`.
3.  **Propagate Error Backwards:** For each hidden layer (moving from back to front):
    *   Calculate the error gradient for each neuron based on the weighted sum of the errors from the layer *in front* of it.
    *   The formula involves the derivative of the activation function.
4.  **Update All Weights:** Once all error gradients are calculated, loop through every neuron in the network and update its weights and bias using the standard rule: `new_weight = old_weight + learning_rate * gradient * input`.


## 4. Activation Functions

The simple step function is not sufficient for backpropagation. We need a differentiable activation function.

*   **Proposed Function: ReLU (Rectified Linear Unit)**
    *   **Formula:** `f(x) = max(0, x)`
    *   **Implementation:** `DUP 0 < IF 0 END`
    *   **Derivative:** `f'(x) = 1 if x > 0, else 0`
    *   **Reasoning:** ReLU is computationally very cheap (no exponentials or divisions) and performs well, making it ideal for the Stern-XT's CPU.

We will need to create two new helper functions in the library: `_relu` and `_relu_derivative`.


## 5. Proposed Library API (`mlnn_lib.stacks`)

The new library will have a clear, high-level API.

*   `DEF MLNN.new_network { (input_size, hidden_layer_sizes_ptr, output_size -- network_ptr) }`
    *   A powerful constructor that takes the number of inputs, a pointer to an array of hidden layer sizes (e.g., `[8, 8]`), and the number of outputs. It will allocate the entire network structure on the heap.

*   `DEF MLNN.predict { (network_ptr, input_array_ptr -- output_array_ptr) }`
    *   Performs the forward pass and returns a pointer to an array containing the final output values.

*   `DEF MLNN.train { (network_ptr, input_array_ptr, target_array_ptr, learning_rate -- ) }`
    *   Executes one full step of the backpropagation algorithm.


## 6. Proof of Concept: Trainer Application

To prove that the `mlnn_lib` is working correctly, the current project will focus on creating a trainer application that solves a classic non-linear problem, such as XOR. This is a task that a single perceptron cannot learn, making it an ideal test case for a multi-layer network.

The trainer application (`mlnn_xor_trainer.stacks`) will:
1.  **Build the Network:** Use `MLNN.new_network` to create a small network suitable for the task, such as a `2-4-1` architecture (2 inputs, 1 hidden layer of 4 neurons, 1 output).
2.  **Train the Network:** Run a training loop for a fixed number of iterations. In each iteration, it will:
    *   Randomly pick one of the four XOR data points (e.g., `[0, 1]`).
    *   Provide the corresponding target output (e.g., `[1]`).
    *   Call `MLNN.train` to adjust the network's weights.
3.  **Test the Network:** After training is complete, run a separate testing loop. This loop will feed all four XOR inputs into the trained network using `MLNN.predict` and report on the accuracy of the outputs.

This focused approach removes the need for `export_weights` and `import_weights` functions for the current project, simplifying the scope. Successfully training a network to solve XOR will be a definitive demonstration of the library's power and correctness.

This design provides a clear roadmap. The next step would be to start implementing the data structure constructors (`MLNN.new_network`) and the ReLU activation function, as they are the first building blocks required for the rest of the library.

<!--
[PROMPT_SUGGESTION]Can you outline the Stacks code for the `MLNN.new_network` constructor function?[/PROMPT_SUGGESTION]
[PROMPT_SUGGESTION]Show me how to implement the ReLU activation function and its derivative in Stacks using the fixed-point library.[/PROMPT_SUGGESTION]
