# ----------------------------------------------------------------------------
# Library: mlnn_lib.stacks
# Description: A Multi-Layer Neural Network library for the Stern-XT platform.
#              Provides functionality to create, train, and run a 3-layer
#              (input, hidden, output) perceptron network.
#
# API:
#   - NN.new_network: Creates a new 3-layer network.
#   - NN.predict: Performs a forward pass (prediction).
#   - NN.train: Performs a single step of backpropagation training.
# ----------------------------------------------------------------------------

USE std_heap
USE fixed_point_lib

# ----------------------------------------------------------------------------
# Helper Variables
#
# These are used internally by the library functions. Due to the global
# scope of Stacks variables, they are defined here.
# ----------------------------------------------------------------------------

VALUE _nn_temp_ptr 0
VALUE _nn_num_neurons 0

VALUE _nn_input_size 0
VALUE _nn_hidden_size 0
VALUE _nn_output_size 0

VALUE _nn_predict_layer_ptr 0
VALUE _nn_predict_input_ptr 0
VALUE _nn_predict_output_ptr 0
VALUE _nn_predict_neuron_idx 0
VALUE _nn_predict_input_idx 0
VALUE _nn_predict_weighted_sum 0



# ----------------------------------------------------------------------------
# HELPER: ARRAY.clear
# Resets the count of an array to zero, effectively clearing it without
# deallocating its memory. This is essential for reusing temporary arrays
# during the training process.
#
# Stack: (array_ptr -- )
# ----------------------------------------------------------------------------
DEF ARRAY.clear {
    # ( array_ptr -- )
    # This function takes an array pointer, calculates the address
    # of its 'count' field (ptr + 1), and writes a 0 to that address.
    # It uses the globally defined _nn_temp_ptr for the operation.
    1 + AS _nn_temp_ptr
    0 AS *_nn_temp_ptr
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.new_layer
# Creates a single layer of neurons.
#
# A layer is an array of perceptron pointers. Each perceptron is an array
# containing its bias and a pointer to its weights array.
#
# Stack: (num_neurons, num_inputs_per_neuron -- layer_ptr)
# ----------------------------------------------------------------------------
VALUE _nn_fill_counter 0
VALUE _nn_perceptron_ptr 0
VALUE _nn_weights_ptr 0


DEF _NN.new_layer {
    # ( num_neurons, num_inputs_per_neuron -- layer_ptr )
    AS _nn_input_size
    AS _nn_num_neurons

    # Create the layer array to hold the perceptron pointers
    _nn_num_neurons NEW.array AS _nn_temp_ptr

    WHILE _nn_num_neurons 0 > DO
        # Create the perceptron array: [bias, weights_ptr]
        2 NEW.array AS _nn_perceptron_ptr

        # Create the weights array for this perceptron
        _nn_input_size NEW.array AS _nn_weights_ptr

        # Initialize all weights with small random fixed-point numbers
        # to break symmetry and enable learning.
        0 AS _nn_fill_counter
        WHILE _nn_fill_counter _nn_input_size < DO
            # Generate a random fixed-point number between -0.1 and 0.1
            RND 200 % 100 - FP.from_int # Random integer from -100 to 99
            1000 FP.from_int            # Divisor
            FP.div                      # Result is a small fraction
            _nn_weights_ptr ARRAY.append
            _nn_fill_counter 1 + AS _nn_fill_counter
        DONE

        # Initialize bias to 0 (as a fixed-point number) and append it.
        0 FP.from_int _nn_perceptron_ptr ARRAY.append
        # Append the pointer to the weights array.
        _nn_weights_ptr _nn_perceptron_ptr ARRAY.append

        # Push the newly created perceptron_ptr onto the layer array
        _nn_perceptron_ptr _nn_temp_ptr ARRAY.append

        _nn_num_neurons 1 - AS _nn_num_neurons
    DONE

    # Return the layer_ptr
    _nn_temp_ptr
}


# ----------------------------------------------------------------------------
# PUBLIC API: NN.new_network
# Creates a new 3-layer neural network.
#
# The network structure is an array of 3 elements:
# [input_size, hidden_layer_ptr, output_layer_ptr]
#
# Stack: (input_size, hidden_size, output_size -- network_ptr)
# ----------------------------------------------------------------------------
VALUE _nn_network_ptr 0
VALUE _nn_hidden_layer_ptr 0
VALUE _nn_output_layer_ptr 0

DEF NN.new_network {
    # ( input_size, hidden_size, output_size -- network_ptr )
    AS _nn_output_size
    AS _nn_hidden_size
    AS _nn_input_size

    # Create the top-level network array to hold the two layer pointers
    3 NEW.array AS _nn_network_ptr

    # Create the hidden layer. Each neuron has `input_size` weights.
    _nn_hidden_size _nn_input_size _NN.new_layer AS _nn_hidden_layer_ptr

    # Create the output layer. Each neuron has `hidden_size` weights,
    # as it takes input from the hidden layer.
    _nn_output_size _nn_hidden_size _NN.new_layer AS _nn_output_layer_ptr

    # Push the layer pointers into the main network array
    _nn_input_size _nn_network_ptr ARRAY.append
    _nn_hidden_layer_ptr _nn_network_ptr ARRAY.append
    _nn_output_layer_ptr _nn_network_ptr ARRAY.append

    # Return the pointer to the fully constructed network
    _nn_network_ptr
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.relu
# The Rectified Linear Unit activation function.
# Formula: f(x) = max(0, x)
#
# Stack: (x -- result)
# ----------------------------------------------------------------------------
DEF _NN.relu {
    # ( x -- result )
    DUP 0 < IF
        DROP # Discard the negative number
        0 FP.from_int # Push 0
    END
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.relu_derivative
# The derivative of the ReLU activation function.
# Formula: f'(x) = 1 if x > 0, else 0
#
# Stack: (x -- result)
# ----------------------------------------------------------------------------
DEF _NN.relu_derivative {
    # ( x -- result )
    0 > IF
        1 FP.from_int # Return 1.0
    ELSE
        0 FP.from_int # Return 0.0
    END
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.forward_pass_layer
# Performs a forward pass for a single layer of the network.
#
# It calculates the weighted sum for each neuron, adds the bias, applies the
# ReLU activation function, and stores the result in an output array.
#
# Stack: (layer_ptr, input_array_ptr -- output_array_ptr)
# ----------------------------------------------------------------------------
DEF _NN.forward_pass_layer {
    # ( layer_ptr, input_array_ptr -- output_array_ptr )
    AS _nn_predict_input_ptr
    AS _nn_predict_layer_ptr

    # Create a new array to store the activations of this layer
    _nn_predict_layer_ptr ARRAY.len NEW.array AS _nn_predict_output_ptr

    # Loop through each neuron in the layer
    0 AS _nn_predict_neuron_idx
    WHILE _nn_predict_neuron_idx _nn_predict_layer_ptr ARRAY.len < DO
        # Get the current neuron (perceptron)
        _nn_predict_layer_ptr _nn_predict_neuron_idx ARRAY.get AS _nn_perceptron_ptr

        # Get the bias and weights_ptr from the perceptron
        _nn_perceptron_ptr 0 ARRAY.get AS _nn_predict_weighted_sum # Start sum with bias
        _nn_perceptron_ptr 1 ARRAY.get AS _nn_weights_ptr

        # Calculate the weighted sum of inputs
        0 AS _nn_predict_input_idx
        WHILE _nn_predict_input_idx _nn_predict_input_ptr ARRAY.len < DO
            # Get weight and input
            _nn_weights_ptr _nn_predict_input_idx ARRAY.get
            _nn_predict_input_ptr _nn_predict_input_idx ARRAY.get

            # Multiply and add to sum
            FP.mul
            _nn_predict_weighted_sum FP.add AS _nn_predict_weighted_sum

            _nn_predict_input_idx 1 + AS _nn_predict_input_idx
        DONE

        # Apply the activation function
        _nn_predict_weighted_sum _NN.relu

        # Append the final activation to this layer's output array
        _nn_predict_output_ptr ARRAY.append

        _nn_predict_neuron_idx 1 + AS _nn_predict_neuron_idx
    DONE

    # Return the pointer to the array of activations
    _nn_predict_output_ptr
}


# ----------------------------------------------------------------------------
# PUBLIC API: NN.predict
# Performs a full forward pass through the 3-layer network.
#
# Stack: (network_ptr, input_array_ptr -- output_array_ptr)
# ----------------------------------------------------------------------------
DEF NN.predict {
    # ( network_ptr, input_array_ptr -- output_array_ptr )
    AS _nn_predict_input_ptr
    AS _nn_network_ptr

    # --- 1. Process Hidden Layer ---
    # Get the hidden layer pointer from the network structure
    _nn_network_ptr 1 ARRAY.get AS _nn_hidden_layer_ptr

    # Perform forward pass on the hidden layer
    _nn_hidden_layer_ptr _nn_predict_input_ptr _NN.forward_pass_layer AS _nn_predict_output_ptr

    # --- 2. Process Output Layer ---
    # The output of the hidden layer becomes the input for the output layer
    _nn_predict_output_ptr AS _nn_predict_input_ptr

    # Get the output layer pointer from the network structure
    _nn_network_ptr 2 ARRAY.get AS _nn_output_layer_ptr

    # Perform forward pass on the output layer
    _nn_output_layer_ptr _nn_predict_input_ptr _NN.forward_pass_layer AS _nn_predict_output_ptr

    # Return the final predictions
    _nn_predict_output_ptr
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.set_weight
# Manually sets a specific weight in a neuron. Useful for testing.
#
# Stack: (value, weight_idx, neuron_ptr -- )
# ----------------------------------------------------------------------------
VALUE _nn_input_idx 0

DEF _NN.set_weight {
    # ( value, weight_idx, neuron_ptr -- )
    AS _nn_perceptron_ptr
    AS _nn_input_idx # Re-using this var for weight_idx
    AS _nn_temp_ptr    # Re-using this var for the value

    # Get the weights_ptr from the perceptron
    _nn_perceptron_ptr 1 ARRAY.get AS _nn_weights_ptr
    _nn_temp_ptr _nn_input_idx _nn_weights_ptr ARRAY.put
}