# ============================================================================
# Multi-Layer Neural Network Library (mlnn_lib)
#
# Provides building blocks for multi-layer perceptrons (MLPs).
#
# Depends on:
#   - std_heap: For dynamic array structures.
#   - fixed_point_lib: For handling fractional numbers.
# ============================================================================

USE std_heap
USE fixed_point_lib

# --- Activation Functions (Internal Helpers) ---

# ----------------------------------------------------------------------------
# DEF _relu (Activation Method)
# Applies the Rectified Linear Unit (ReLU) activation function.
# f(x) = max(0, x)
#
# @param x A fixed-point number.
# @returns The result of the ReLU function (a fixed-point number).
# ----------------------------------------------------------------------------
DEF _relu {
    # The formula is max(0, x). This version does NOT consume the input.
    # ( x -- result x )
    DUP DUP 0 < IF DROP 0 FP.from_int END SWAP
}

# ----------------------------------------------------------------------------
# DEF _relu_derivative
# Calculates the derivative of the ReLU function.
# f'(x) = 1 if x > 0, else 0
#
# @param x A fixed-point number.
# @returns The derivative (1 or 0 as a fixed-point number), leaving the
#          original input on the stack.
# ----------------------------------------------------------------------------
DEF _relu_derivative {
    # The formula is 1 if x > 0, else 0. This version does NOT consume the input.
    # ( x -- derivative x )
    DUP 0 > IF
        1 FP.from_int
    ELSE
        0 FP.from_int
    END
    SWAP
}

# ----------------------------------------------------------------------------
# DEF _new_perceptron (Internal Helper)
# Creates a single perceptron with a given number of inputs.
# This is based on the original nn_lib.stacks implementation.
#
# @param num_inputs The number of weights to create for this perceptron.
# @returns perceptron_ptr A pointer to the newly created perceptron structure.
# ----------------------------------------------------------------------------
DEF _new_perceptron {
    AS num_inputs

    # 1. Create the weights array and initialize with small random values
    num_inputs NEW.array AS weights_ptr
    0 AS np_i
    WHILE np_i num_inputs < DO
        RND 200 % 100 - FP.from_int # Random value between -0.1 and 0.1
        1000 FP.from_int
        FP.div
        weights_ptr ARRAY.append
        np_i 1 + AS np_i
    DONE

    # 2. Create the main perceptron array [bias, weights_ptr]
    2 NEW.array AS perceptron_ptr

    # 3. Initialize bias to 0 and append
    0 FP.from_int perceptron_ptr ARRAY.append

    # 4. Append the pointer to the weights array
    weights_ptr perceptron_ptr ARRAY.append

    perceptron_ptr # Return the pointer
}

# ----------------------------------------------------------------------------
# DEF _predict_layer (Internal Helper)
# Performs a forward pass on a single layer of the network.
#
# @param layer_ptr A pointer to a layer structure.
# @param input_ptr A pointer to an array of input values for this layer.
# @param output_ptr A pointer to an array where the output values will be written.
# ----------------------------------------------------------------------------
DEF _predict_layer {
    AS output_ptr
    AS input_ptr
    AS layer_ptr

    VALUE _temp_ptr 0

    # Set the output buffer's count to 0 to effectively clear it for new data.
    output_ptr 1 + AS _temp_ptr
    0 AS *_temp_ptr

    0 AS pl_i
    layer_ptr ARRAY.len AS num_neurons
    WHILE pl_i num_neurons < DO
        # For each neuron in the layer...
        pl_i layer_ptr ARRAY.get AS perceptron_ptr

        # --- Calculate weighted sum (dot product + bias) ---
        0 perceptron_ptr ARRAY.get AS bias
        1 perceptron_ptr ARRAY.get AS weights_ptr
        bias AS weighted_sum

        0 AS pl_j
        input_ptr ARRAY.len AS num_inputs
        WHILE pl_j num_inputs < DO
            pl_j input_ptr ARRAY.get pl_j weights_ptr ARRAY.get FP.mul weighted_sum FP.add AS weighted_sum
            pl_j 1 + AS pl_j
        DONE

        # --- Apply activation function and store result ---
        # Call our non-consuming _relu function, then DROP the original
        # value it leaves on the stack, as we only need the result.
        weighted_sum _relu DROP output_ptr ARRAY.append
        pl_i 1 + AS pl_i
    DONE
}


# --- Public API ---

# ----------------------------------------------------------------------------
# DEF NN.new_network (Create Network Method)
# Creates a new multi-layer network structure on the heap.
#
# @param input_size The number of input neurons.
# @param hidden_layer_sizes_ptr A pointer to an array containing the size
#                               of each hidden layer (e.g., [8, 8]).
# @param output_size The number of output neurons.
# @returns network_ptr A pointer to the newly created network structure.
# ----------------------------------------------------------------------------
DEF NN.new_network {
    # ( output_size hidden_layer_sizes_ptr input_size -- network_ptr )
    AS input_size
    AS hidden_layer_sizes_ptr
    AS output_size

    hidden_layer_sizes_ptr ARRAY.len AS num_hidden_layers

    # The network is an array of layers. Total layers = hidden layers + 1 output layer.
    num_hidden_layers 1 + NEW.array AS network_ptr

    # --- Build Hidden Layers ---
    input_size AS num_inputs_for_current_layer
    0 AS nn_i
    WHILE nn_i num_hidden_layers < DO
        # Get the number of neurons for this hidden layer
        nn_i hidden_layer_sizes_ptr ARRAY.get AS num_neurons_in_layer

        # Create the layer array, which will hold pointers to perceptrons
        num_neurons_in_layer NEW.array AS layer_ptr

        # Create each perceptron in the layer
        0 AS nn_j
        WHILE nn_j num_neurons_in_layer < DO
            num_inputs_for_current_layer _new_perceptron layer_ptr ARRAY.append
            nn_j 1 + AS nn_j
        DONE

        # Add the completed layer to the network
        layer_ptr network_ptr ARRAY.append

        # The number of inputs for the *next* layer is the number of neurons in *this* layer.
        num_neurons_in_layer AS num_inputs_for_current_layer
        nn_i 1 + AS nn_i
    DONE

    # --- Build Output Layer ---
    # The number of inputs for the output layer is the number of neurons in the last hidden layer.
    output_size NEW.array AS output_layer_ptr

    0 AS nn_j_out
    WHILE nn_j_out output_size < DO
        num_inputs_for_current_layer _new_perceptron output_layer_ptr ARRAY.append
        nn_j_out 1 + AS nn_j_out
    DONE

    # Add the completed output layer to the network
    output_layer_ptr network_ptr ARRAY.append

    network_ptr # Return the pointer to the whole network
}

# ----------------------------------------------------------------------------
# DEF NN.predict (Predict Method)
# Performs a full forward pass of the network to generate a prediction.
#
# @param network_ptr A pointer to the network structure.
# @param input_array_ptr A pointer to an array of input values.
# @returns output_array_ptr A pointer to an array of the final output values.
# ----------------------------------------------------------------------------
DEF NN.predict {
    # ( user_input_ptr network_ptr -- final_output_ptr )
    AS network_ptr
    AS user_input_ptr

    network_ptr ARRAY.len AS num_layers

    # --- Pre-allocate Buffers to prevent memory leaks ---
    # We use a "ping-pong" buffer strategy. We need two buffers that are
    # large enough to hold the outputs of the largest layer.
    # For this POC, we can assume a max layer size (e.g., 16).
    16 NEW.array AS predict_buffer_a
    16 NEW.array AS predict_buffer_b

    # The initial input is the user's data.
    user_input_ptr AS current_input_ptr

    0 AS p_i
    WHILE p_i num_layers < DO
        # Get the current layer from the network
        p_i network_ptr ARRAY.get AS layer_ptr

        # Determine which buffer to use as the output destination for this layer.
        # We alternate between buffer_a and buffer_b based on the loop index.
        p_i 2 % 0 == IF predict_buffer_a ELSE predict_buffer_b END
        AS output_buffer_ptr

        # Process the layer, writing the output into the 'to_ptr' buffer.
        current_input_ptr layer_ptr output_buffer_ptr _predict_layer

        # The output buffer from this iteration becomes the input for the next.
        output_buffer_ptr AS current_input_ptr
        p_i 1 + AS p_i
    DONE

    # After the loop, current_input_ptr holds the pointer to the buffer
    # containing the final result of the last layer.
    current_input_ptr
}

# ----------------------------------------------------------------------------
# DEF NN.train (Train Step Method)
# Executes one full step of the backpropagation algorithm.
#
# @param network_ptr A pointer to the network structure.
# @param input_array_ptr A pointer to an array of input values.
# @param target_array_ptr A pointer to an array of target output values.
# @param learning_rate A fixed-point value for the learning step size.
# ----------------------------------------------------------------------------
DEF NN.train {
    # ( learning_rate target_array_ptr user_input_ptr network_ptr -- )
    AS network_ptr
    AS user_input_ptr
    AS target_array_ptr
    AS learning_rate

    # VALUE _temp_ptr 0     # does already exist, thats we reuse 
    network_ptr ARRAY.len AS num_layers

    # --- Stage 1: Forward Pass with Storage ---
    # We must store the activations from every layer to use during backpropagation.
    # We will create an array of pointers. Each pointer will point to an
    # array of activations for one layer.

    # `all_activations` will store [user_input_ptr, layer1_activations_ptr, layer2_activations_ptr, ...]
    num_layers 1 + NEW.array AS all_activations_ptr
    user_input_ptr all_activations_ptr ARRAY.append

    # We also need to store the pre-activation weighted sums for calculating derivatives.
    num_layers NEW.array AS all_weighted_sums_ptr

    # --- Pre-allocate Buffers for the forward pass ---
    16 NEW.array AS train_activations_a
    16 NEW.array AS train_activations_b
    16 NEW.array AS train_sums_a
    16 NEW.array AS train_sums_b

    user_input_ptr AS current_input_ptr

    0 AS fwd_i
    WHILE fwd_i num_layers < DO
        # Get the current layer from the network
        fwd_i network_ptr ARRAY.get AS layer_ptr

        # Determine which buffer to use as the output destination.
        fwd_i 2 % 0 == IF train_activations_a ELSE train_activations_b END
        AS output_buffer_ptr

        # Determine which buffer to use for the weighted sums.
        fwd_i 2 % 0 == IF train_sums_a ELSE train_sums_b END
        AS sums_buffer_ptr

        # --- Modified _predict_layer logic ---
        # This is similar to _predict_layer, but it stores both the
        # weighted sums and the final activations.
        output_buffer_ptr 1 + AS _temp_ptr 0 AS *_temp_ptr # Clear output buffer
        sums_buffer_ptr 1 +   AS _temp_ptr 0 AS *_temp_ptr # Clear the sums buffer

        0 AS fwd_j
        layer_ptr ARRAY.len AS num_neurons
        WHILE fwd_j num_neurons < DO
            fwd_j layer_ptr ARRAY.get AS perceptron_ptr
            0 perceptron_ptr ARRAY.get AS bias
            1 perceptron_ptr ARRAY.get AS weights_ptr
            bias AS weighted_sum
            0 AS fwd_k
            current_input_ptr ARRAY.len AS num_inputs
            WHILE fwd_k num_inputs < DO
                fwd_k current_input_ptr ARRAY.get fwd_k weights_ptr ARRAY.get FP.mul weighted_sum FP.add AS weighted_sum
                fwd_k 1 + AS fwd_k
            DONE
            weighted_sum sums_buffer_ptr ARRAY.append # Store pre-activation sum
            weighted_sum _relu DROP output_buffer_ptr ARRAY.append
            fwd_j 1 + AS fwd_j
        DONE

        # Store the pointers to the buffers we just filled for the backward pass
        sums_buffer_ptr all_weighted_sums_ptr ARRAY.append
        output_buffer_ptr all_activations_ptr ARRAY.append

        # The output from this layer is the input for the next
        output_buffer_ptr AS current_input_ptr
        fwd_i 1 + AS fwd_i
    DONE

    # --- Stage 2: Backward Pass ---
    # At this point:
    # - `all_activations_ptr` holds the output of every layer.
    # - `all_weighted_sums_ptr` holds the pre-activation sum of every neuron.
    # We now have all the data needed to calculate the gradients and update the weights.

    # We calculate the error gradients, starting from the output layer and
    # working backwards. Let's create a buffer to hold these gradients.
    16 NEW.array AS gradient_buffer
    16 NEW.array AS new_gradient_buffer # Pre-allocate the second gradient buffer

    # --- Calculate Gradients for the Output Layer ---
    num_layers 1 - AS last_layer_index

    # Get the data for the last layer from our storage arrays
    last_layer_index all_weighted_sums_ptr ARRAY.get AS last_weighted_sums_ptr
    num_layers all_activations_ptr ARRAY.get AS last_activations_ptr

    0 AS bwd_j
    last_activations_ptr ARRAY.len AS num_output_neurons
    WHILE bwd_j num_output_neurons < DO
        # For each neuron in the output layer...

        # 1. Calculate error: (target - actual)
        bwd_j target_array_ptr ARRAY.get AS target_val
        bwd_j last_activations_ptr ARRAY.get AS actual_val
        target_val actual_val FP.sub AS error

        # 2. Get the derivative of the activation function
        bwd_j last_weighted_sums_ptr ARRAY.get _relu_derivative DROP AS derivative

        # 3. Calculate final gradient: error * derivative
        error derivative FP.mul AS gradient
        gradient gradient_buffer ARRAY.append

        bwd_j 1 + AS bwd_j
    DONE

    # --- Update Weights and Biases for the Output Layer ---
    last_layer_index network_ptr ARRAY.get AS output_layer_ptr
    last_layer_index all_activations_ptr ARRAY.get AS inputs_to_output_layer_ptr

    0 AS bwd_j_upd
    num_output_neurons # This is still on the stack from the previous loop
    WHILE bwd_j_upd SWAP DUP < DO
        # For each neuron in the output layer...
        bwd_j_upd output_layer_ptr ARRAY.get AS perceptron_ptr
        bwd_j_upd gradient_buffer ARRAY.get AS gradient

        # 1. Update the bias
        # new_bias = old_bias + learning_rate * gradient
        0 perceptron_ptr ARRAY.get AS old_bias
        learning_rate gradient FP.mul AS bias_adjustment
        old_bias bias_adjustment FP.add AS new_bias
        new_bias 0 perceptron_ptr ARRAY.put

        # 2. Update the weights
        1 perceptron_ptr ARRAY.get AS weights_ptr
        0 AS bwd_l_upd
        weights_ptr ARRAY.len AS num_weights
        WHILE bwd_l_upd num_weights < DO
            # For each weight in the neuron...
            
            # Get the corresponding input that was fed to this neuron
            bwd_l_upd inputs_to_output_layer_ptr ARRAY.get AS input_val

            # Calculate weight_delta = learning_rate * gradient * input
            bias_adjustment input_val FP.mul AS weight_delta

            # new_weight = old_weight + weight_delta
            bwd_l_upd weights_ptr ARRAY.get AS old_weight
            old_weight weight_delta FP.add AS new_weight
            new_weight bwd_l_upd weights_ptr ARRAY.put

            bwd_l_upd 1 + AS bwd_l_upd
        DONE

        bwd_j_upd 1 + AS bwd_j_upd
    DONE
    DROP # Clean up the loop variable

    # --- Propagate Gradients to Hidden Layers ---
    # We loop backwards from the last hidden layer to the first.
    last_layer_index 1 - AS current_layer_index
    WHILE current_layer_index -1 > DO
        # Get the data for the current layer and the layer *after* it.
        current_layer_index network_ptr ARRAY.get AS current_layer_ptr
        current_layer_index 1 + network_ptr ARRAY.get AS next_layer_ptr
        current_layer_index all_weighted_sums_ptr ARRAY.get AS current_weighted_sums_ptr
        current_layer_index all_activations_ptr ARRAY.get AS inputs_to_current_layer_ptr

        # Clear the new_gradient_buffer before use
        new_gradient_buffer 1 + AS _temp_ptr 0 AS *_temp_ptr

        0 AS bwd_h_j
        current_layer_ptr ARRAY.len AS num_neurons_in_layer
        WHILE bwd_h_j num_neurons_in_layer < DO
            # For each neuron in the current hidden layer...

            # 1. Calculate the error contribution from the next layer.
            # This is the sum of (gradient_from_next_layer * weight_of_connection)
            0 FP.from_int AS error_sum
            0 AS bwd_h_k
            next_layer_ptr ARRAY.len AS num_neurons_in_next_layer
            WHILE bwd_h_k num_neurons_in_next_layer < DO
                # Get the gradient of a neuron in the next layer
                bwd_h_k gradient_buffer ARRAY.get AS next_layer_gradient

                # Get the weight connecting this current neuron (j) to that next neuron (k)
                bwd_h_k next_layer_ptr ARRAY.get 1 SWAP ARRAY.get bwd_h_j SWAP ARRAY.get AS connection_weight

                # Add to the sum
                next_layer_gradient connection_weight FP.mul error_sum FP.add AS error_sum

                bwd_h_k 1 + AS bwd_h_k
            DONE

            # 2. Get the derivative for the current neuron
            bwd_h_j current_weighted_sums_ptr ARRAY.get _relu_derivative DROP AS derivative

            # 3. Calculate the final gradient for this hidden neuron
            error_sum derivative FP.mul AS gradient
            gradient new_gradient_buffer ARRAY.append

            bwd_h_j 1 + AS bwd_h_j
        DONE

        # Now that we have the gradients for this hidden layer, update its weights.
        # This is the same logic as the output layer update.
        0 AS bwd_h_j_upd
        num_neurons_in_layer # Still on the stack
        WHILE bwd_h_j_upd SWAP DUP < DO
            bwd_h_j_upd current_layer_ptr ARRAY.get AS perceptron_ptr
            bwd_h_j_upd new_gradient_buffer ARRAY.get AS gradient

            0 perceptron_ptr ARRAY.get AS old_bias
            learning_rate gradient FP.mul AS bias_adjustment
            old_bias bias_adjustment FP.add AS new_bias
            new_bias 0 perceptron_ptr ARRAY.put

            1 perceptron_ptr ARRAY.get AS weights_ptr
            0 AS bwd_h_l_upd
            weights_ptr ARRAY.len AS num_weights
            WHILE bwd_h_l_upd num_weights < DO
                bwd_h_l_upd inputs_to_current_layer_ptr ARRAY.get AS input_val
                bias_adjustment input_val FP.mul AS weight_delta
                bwd_h_l_upd weights_ptr ARRAY.get AS old_weight
                old_weight weight_delta FP.add AS new_weight
                new_weight bwd_h_l_upd weights_ptr ARRAY.put
                bwd_h_l_upd 1 + AS bwd_h_l_upd
            DONE
            bwd_h_j_upd 1 + AS bwd_h_j_upd
        DONE
        DROP # Clean up loop variable

        # --- Ping-Pong the gradient buffers for the next backward iteration ---
        # The buffer we just wrote to ('new_gradient_buffer') becomes the
        # source of gradients ('gradient_buffer') for the next layer back.
        gradient_buffer AS temp_grad_ptr
        new_gradient_buffer AS gradient_buffer
        temp_grad_ptr AS new_gradient_buffer

        current_layer_index 1 - AS current_layer_index
    DONE
}