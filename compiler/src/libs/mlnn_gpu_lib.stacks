# ----------------------------------------------------------------------------
# Library: mlnn_gpu_lib.stacks
# Description: GPU-Accelerated Multi-Layer Neural Network library.
#              Drop-in replacement for mlnn_lib.stacks.
#
# API:
#   - NN.new_network: Creates a new 3-layer network (using Matrices).
#   - NN.predict: Performs a forward pass using GPU acceleration.
#   - NN.train: (TODO) Performs backpropagation.
# ----------------------------------------------------------------------------

USE std_heap
USE fixed_point_lib
USE gpu_lib

# ----------------------------------------------------------------------------
# Global Variables
# ----------------------------------------------------------------------------

VALUE _nn_tdl_ptr 0
VALUE _nn_scale 10000  # Default scale, should match FP.set_scale

VALUE _nn_temp_ptr 0
VALUE _nn_fill_counter 0
VALUE _nn_row_counter 0
VALUE _nn_col_counter 0
VALUE _nn_train_k 0

# Network Dimensions (Persistent during creation)
VALUE _nn_net_input_size 0
VALUE _nn_net_hidden_size 0
VALUE _nn_net_output_size 0

# Network Pointers
VALUE _nn_network_ptr 0
VALUE _nn_hidden_layer_ptr 0
VALUE _nn_output_layer_ptr 0

# Prediction Pointers
VALUE _nn_predict_input_ptr 0
VALUE _nn_predict_layer_ptr 0
VALUE _nn_predict_output_ptr 0
VALUE _nn_weights_ptr 0
VALUE _nn_bias_ptr 0

# Temporary Matrices for Prediction (Pre-allocated)
VALUE _nn_mat_input_wrapper 0
VALUE _nn_mat_hidden_activations 0
VALUE _nn_mat_output_activations 0

# Temporary Matrices for Training (Pre-allocated)
VALUE _nn_mat_target 0
VALUE _nn_mat_output_error 0
VALUE _nn_mat_output_deriv 0
VALUE _nn_mat_output_delta 0

VALUE _nn_mat_weights_ho_trans 0
VALUE _nn_mat_hidden_error 0
VALUE _nn_mat_hidden_deriv 0
VALUE _nn_mat_hidden_delta 0

VALUE _nn_mat_hidden_act_trans 0
VALUE _nn_mat_grad_ho 0
VALUE _nn_mat_input_trans 0
VALUE _nn_mat_grad_ih 0

VALUE _nn_mat_bias_input 0

STRING bias_val "0.1"

# ----------------------------------------------------------------------------
# Configuration
# ----------------------------------------------------------------------------

DEF NN.set_scale {
    AS _nn_scale
}

# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.new_layer
# Creates a layer structure containing two Matrices: Weights and Bias.
#
# Weights Matrix: [Inputs x Neurons]
# Bias Matrix:    [1 x Neurons]
#
# Stack: ( num_neurons, num_inputs_per_neuron -- layer_ptr )
# ----------------------------------------------------------------------------
DEF _NN.new_layer {
    AS _nn_col_counter # Inputs (Rows of weights)
    AS _nn_row_counter # Neurons (Cols of weights)

    # 1. Create Layer Container (Array of 2 pointers)
    2 NEW.array AS _nn_temp_ptr

    # 2. Create Weights Matrix (Inputs x Neurons)
    # Standard Dot Product: (1 x In) . (In x Neurons) = (1 x Neurons)
    _nn_col_counter _nn_row_counter NEW.matrix AS _nn_weights_ptr

    # Initialize Weights with Random Values
    # Loop Rows (Inputs)
    1 AS _nn_fill_counter # row idx
    WHILE _nn_fill_counter _nn_col_counter 1 + < DO
        
        # Loop Cols (Neurons)
        1 AS _nn_predict_layer_ptr # col idx (reusing var)
        WHILE _nn_predict_layer_ptr _nn_row_counter 1 + < DO
            
            # Generate Random: (RND % 200 - 100) / 1000
            RND 200 % 100 - FP.from_int
            1000 FP.from_int
            FP.div
            
            # Put into Matrix
            _nn_fill_counter _nn_predict_layer_ptr _nn_weights_ptr MATRIX.put

            _nn_predict_layer_ptr 1 + AS _nn_predict_layer_ptr
        DONE

        _nn_fill_counter 1 + AS _nn_fill_counter
    DONE

    # 3. Create Bias Matrix (1 x Neurons)
    1 _nn_row_counter NEW.matrix AS _nn_bias_ptr

    # Initialize Bias to 0.1
    1 AS _nn_fill_counter # col idx
    WHILE _nn_fill_counter _nn_row_counter 1 + < DO
        &bias_val FP.from_string
        1 _nn_fill_counter _nn_bias_ptr MATRIX.put
        _nn_fill_counter 1 + AS _nn_fill_counter
    DONE

    # 4. Store in Layer Container
    _nn_weights_ptr _nn_temp_ptr ARRAY.append
    _nn_bias_ptr _nn_temp_ptr ARRAY.append

    _nn_temp_ptr
}


# ----------------------------------------------------------------------------
# PUBLIC API: NN.new_network
# Creates a new 3-layer neural network optimized for GPU.
#
# Stack: (input_size, hidden_size, output_size -- network_ptr)
# ----------------------------------------------------------------------------
DEF NN.new_network {
    AS _nn_net_output_size
    AS _nn_net_hidden_size
    AS _nn_net_input_size

    # Allocate TDL for GPU operations (6 words)
    6 NEW.list AS _nn_tdl_ptr

    # Create Network Container
    3 NEW.array AS _nn_network_ptr

    # Create Hidden Layer (Input x Hidden)
    _nn_net_hidden_size _nn_net_input_size _NN.new_layer AS _nn_hidden_layer_ptr

    # Create Output Layer (Hidden x Output)
    _nn_net_output_size _nn_net_hidden_size _NN.new_layer AS _nn_output_layer_ptr

    # Store in Network
    _nn_net_input_size _nn_network_ptr ARRAY.append # Store Input Size
    _nn_hidden_layer_ptr _nn_network_ptr ARRAY.append
    _nn_output_layer_ptr _nn_network_ptr ARRAY.append

    # Pre-allocate Matrices for Forward Pass
    # Input Wrapper (1 x Input)
    1 _nn_net_input_size NEW.matrix AS _nn_mat_input_wrapper
    
    # Hidden Activations (1 x Hidden)
    1 _nn_net_hidden_size NEW.matrix AS _nn_mat_hidden_activations

    # Output Activations (1 x Output)
    1 _nn_net_output_size NEW.matrix AS _nn_mat_output_activations

    # --- Training Matrices ---
    
    # Target & Error (1 x Output)
    1 _nn_net_output_size NEW.matrix AS _nn_mat_target
    1 _nn_net_output_size NEW.matrix AS _nn_mat_output_error
    1 _nn_net_output_size NEW.matrix AS _nn_mat_output_deriv
    1 _nn_net_output_size NEW.matrix AS _nn_mat_output_delta

    # Hidden Backprop (1 x Hidden)
    1 _nn_net_hidden_size NEW.matrix AS _nn_mat_hidden_error
    1 _nn_net_hidden_size NEW.matrix AS _nn_mat_hidden_deriv
    1 _nn_net_hidden_size NEW.matrix AS _nn_mat_hidden_delta

    # Transposed Weights HO (Output x Hidden)
    _nn_net_output_size _nn_net_hidden_size NEW.matrix AS _nn_mat_weights_ho_trans

    # Transposed Activations & Inputs
    _nn_net_hidden_size 1 NEW.matrix AS _nn_mat_hidden_act_trans # Hidden x 1
    _nn_net_input_size 1 NEW.matrix AS _nn_mat_input_trans     # Input x 1

    # Gradients (Same size as Weights)
    # Grad HO (Hidden x Output)
    _nn_net_hidden_size _nn_net_output_size NEW.matrix AS _nn_mat_grad_ho
    # Grad IH (Input x Hidden)
    _nn_net_input_size _nn_net_hidden_size NEW.matrix AS _nn_mat_grad_ih

    # Bias Input Wrapper (1 x 1) - Always holds 1.0
    1 1 NEW.matrix AS _nn_mat_bias_input
    _nn_scale 1 1 _nn_mat_bias_input MATRIX.put

    _nn_network_ptr
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.forward_pass_layer
# Performs a forward pass using GPU acceleration.
#
# Logic:
#   1. DOT:  Input . Weights -> Output
#   2. ADD:  Output + Bias   -> Output
#   3. RELU: Relu(Output)    -> Output
#
# Stack: ( layer_ptr, input_matrix_ptr, output_matrix_ptr -- )
# ----------------------------------------------------------------------------
DEF _NN.forward_pass_layer {
    AS _nn_predict_output_ptr
    AS _nn_predict_input_ptr
    AS _nn_predict_layer_ptr

    # Unpack Layer
    _nn_predict_layer_ptr 0 ARRAY.get AS _nn_weights_ptr
    _nn_predict_layer_ptr 1 ARRAY.get AS _nn_bias_ptr

    # 1. Matrix Multiplication (DOT)
    # Result = (Input . Weights) / Scale
    _nn_predict_input_ptr
    _nn_weights_ptr
    _nn_predict_output_ptr
    _nn_scale
    MAT.DOT
    _nn_tdl_ptr
    GPU.tdl
    _nn_tdl_ptr GPU.exec DROP

    # 2. Add Bias (ADD)
    # Result = Result + Bias
    _nn_predict_output_ptr
    _nn_bias_ptr
    _nn_predict_output_ptr
    0 # Scale ignored
    MAT.ADD
    _nn_tdl_ptr
    GPU.tdl
    _nn_tdl_ptr GPU.exec DROP

    # 3. Activation Function (RELU)
    # Result = Max(0, Result)
    _nn_predict_output_ptr
    0 # B ignored
    _nn_predict_output_ptr
    0 # Scale ignored
    MAT.RELU
    _nn_tdl_ptr
    GPU.tdl
    _nn_tdl_ptr GPU.exec DROP
}


# ----------------------------------------------------------------------------
# PUBLIC API: NN.predict
# Performs a full forward pass.
# Handles conversion from Input Array to Input Matrix.
#
# Stack: (network_ptr, input_array_ptr -- output_matrix_ptr)
# ----------------------------------------------------------------------------
DEF NN.predict {
    AS _nn_predict_input_ptr # This is an Array
    AS _nn_network_ptr

    # 1. Convert Input Array to Input Matrix
    # We copy data from the array to our pre-allocated matrix wrapper
    0 AS _nn_fill_counter
    WHILE _nn_fill_counter _nn_predict_input_ptr ARRAY.len < DO
        _nn_predict_input_ptr _nn_fill_counter ARRAY.get
        1 _nn_fill_counter 1 + _nn_mat_input_wrapper MATRIX.put
        _nn_fill_counter 1 + AS _nn_fill_counter
    DONE

    # 2. Hidden Layer Pass
    _nn_network_ptr 1 ARRAY.get AS _nn_hidden_layer_ptr
    _nn_hidden_layer_ptr _nn_mat_input_wrapper _nn_mat_hidden_activations _NN.forward_pass_layer

    # 3. Output Layer Pass
    _nn_network_ptr 2 ARRAY.get AS _nn_output_layer_ptr
    _nn_output_layer_ptr _nn_mat_hidden_activations _nn_mat_output_activations _NN.forward_pass_layer

    # Return the Output Matrix
    # Note: Since a 1xN Matrix has the same memory layout as an Array (mostly),
    # ARRAY.get will work on this pointer if the user treats it as an array.
    _nn_mat_output_activations
}


# ----------------------------------------------------------------------------
# PRIVATE HELPER: _NN.populate_deriv_matrix
# Calculates the derivative of ReLU for a given activation matrix.
# Since GPU lacks a "Step" function, we do this on CPU.
# Logic: If val > 0, result = 1.0 (scale), else 0.
#
# Stack: ( activation_matrix_ptr, deriv_matrix_ptr -- )
# ----------------------------------------------------------------------------
DEF _NN.populate_deriv_matrix {
    AS _nn_temp_ptr # deriv ptr
    AS _nn_predict_layer_ptr # activation ptr (reusing var)

    # Get dimensions (assuming 1 x N)
    _nn_predict_layer_ptr ARRAY.len AS _nn_col_counter # Cols

    1 AS _nn_fill_counter
    WHILE _nn_fill_counter _nn_col_counter 1 + < DO
        # Read Activation
        1 _nn_fill_counter _nn_predict_layer_ptr MATRIX.get
        
        # Check > 0
        0 > IF
            _nn_scale # 1.0
        ELSE
            0
        END

        # Write to Deriv Matrix
        1 _nn_fill_counter _nn_temp_ptr MATRIX.put

        _nn_fill_counter 1 + AS _nn_fill_counter
    DONE
}


# ----------------------------------------------------------------------------
# PUBLIC API: NN.train
# Performs Backpropagation.
#
# Stack: (network_ptr, input_array_ptr, target_array_ptr, learning_rate -- )
# ----------------------------------------------------------------------------
DEF NN.train {
    AS _nn_train_k # Learning Rate (reusing var temporarily)
    AS _nn_predict_output_ptr # Target Array (reusing var)
    AS _nn_predict_input_ptr # Input Array
    AS _nn_network_ptr

    # 1. Calculate Gradient Scale K
    # We use the DOT product scale to handle Learning Rate.
    # K = (Scale * Scale) / LearningRate
    # Example: Scale=10, LR=1 (0.1). K = 100/1 = 100.
    # Result = (A . B) / 100 -> (A . B) / 10 * 0.1. Correct.
    _nn_scale _nn_train_k FP.div AS _nn_train_k

    # 2. Populate Input Matrix
    0 AS _nn_fill_counter
    WHILE _nn_fill_counter _nn_predict_input_ptr ARRAY.len < DO
        _nn_predict_input_ptr _nn_fill_counter ARRAY.get
        1 _nn_fill_counter 1 + _nn_mat_input_wrapper MATRIX.put
        _nn_fill_counter 1 + AS _nn_fill_counter
    DONE

    # 3. Populate Target Matrix
    0 AS _nn_fill_counter
    WHILE _nn_fill_counter _nn_predict_output_ptr ARRAY.len < DO
        _nn_predict_output_ptr _nn_fill_counter ARRAY.get
        1 _nn_fill_counter 1 + _nn_mat_target MATRIX.put
        _nn_fill_counter 1 + AS _nn_fill_counter
    DONE

    # 4. Forward Pass (Input -> Hidden -> Output)
    _nn_network_ptr 1 ARRAY.get AS _nn_hidden_layer_ptr
    _nn_hidden_layer_ptr _nn_mat_input_wrapper _nn_mat_hidden_activations _NN.forward_pass_layer

    _nn_network_ptr 2 ARRAY.get AS _nn_output_layer_ptr
    _nn_output_layer_ptr _nn_mat_hidden_activations _nn_mat_output_activations _NN.forward_pass_layer

    # 5. Output Deltas
    # Error = Target - Output
    _nn_mat_target _nn_mat_output_activations _nn_mat_output_error 0 MAT.SUB _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP
    
    # Deriv = f'(Output) (CPU)
    _nn_mat_output_activations _nn_mat_output_deriv _NN.populate_deriv_matrix

    # Delta = Error * Deriv
    _nn_mat_output_error _nn_mat_output_deriv _nn_mat_output_delta _nn_scale MAT.MUL _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # 6. Hidden Deltas
    # Transpose Output Weights
    _nn_output_layer_ptr 0 ARRAY.get AS _nn_weights_ptr
    _nn_weights_ptr 0 _nn_mat_weights_ho_trans 0 MAT.TRANS _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # Hidden Error = Output Delta . Weights_HO_T
    _nn_mat_output_delta _nn_mat_weights_ho_trans _nn_mat_hidden_error _nn_scale MAT.DOT _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # Deriv = f'(Hidden) (CPU)
    _nn_mat_hidden_activations _nn_mat_hidden_deriv _NN.populate_deriv_matrix

    # Delta = Hidden Error * Deriv
    _nn_mat_hidden_error _nn_mat_hidden_deriv _nn_mat_hidden_delta _nn_scale MAT.MUL _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # 7. Update Output Layer Weights
    # Grad = (Hidden_Act_T . Output_Delta) * LR
    # We use _nn_train_k as scale to apply LR
    _nn_mat_hidden_activations 0 _nn_mat_hidden_act_trans 0 MAT.TRANS _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP
    
    _nn_mat_hidden_act_trans _nn_mat_output_delta _nn_mat_grad_ho _nn_train_k MAT.DOT _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # Weights = Weights + Grad
    _nn_weights_ptr _nn_mat_grad_ho _nn_weights_ptr 0 MAT.ADD _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # Update Output Bias
    # Bias_Grad = (Bias_Input . Output_Delta) * LR
    # Note: Reusing _nn_mat_output_error as temp buffer for bias grad (1 x Output)
    _nn_mat_bias_input _nn_mat_output_delta _nn_mat_output_error _nn_train_k MAT.DOT _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP
    
    _nn_output_layer_ptr 1 ARRAY.get AS _nn_bias_ptr
    _nn_bias_ptr _nn_mat_output_error _nn_bias_ptr 0 MAT.ADD _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # 8. Update Hidden Layer Weights
    # Grad = (Input_T . Hidden_Delta) * LR
    _nn_mat_input_wrapper 0 _nn_mat_input_trans 0 MAT.TRANS _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    _nn_mat_input_trans _nn_mat_hidden_delta _nn_mat_grad_ih _nn_train_k MAT.DOT _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # Weights = Weights + Grad
    _nn_hidden_layer_ptr 0 ARRAY.get AS _nn_weights_ptr
    _nn_weights_ptr _nn_mat_grad_ih _nn_weights_ptr 0 MAT.ADD _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    # Update Hidden Bias
    # Bias_Grad = (Bias_Input . Hidden_Delta) * LR
    # Reusing _nn_mat_hidden_error as temp buffer
    _nn_mat_bias_input _nn_mat_hidden_delta _nn_mat_hidden_error _nn_train_k MAT.DOT _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP

    _nn_hidden_layer_ptr 1 ARRAY.get AS _nn_bias_ptr
    _nn_bias_ptr _nn_mat_hidden_error _nn_bias_ptr 0 MAT.ADD _nn_tdl_ptr GPU.tdl _nn_tdl_ptr GPU.exec DROP
}